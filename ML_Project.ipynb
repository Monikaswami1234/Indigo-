{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "67NQN5KX2AMe",
        "yiiVWRdJDDil",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - IndiGO project\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ï¸âƒ£ Data Loading & First View\n",
        "The dataset contains 131,895 rows and 17 columns.\n",
        "It has several missing values in many columns.\n",
        "It includes airline reviews with ratings on various aspects like seat comfort, cabin service, food, entertainment, etc.\n",
        "Some columns like review_date and date_flown might need date formatting.\n",
        "\n",
        "2ï¸âƒ£ Data Information & Understanding Variables\n",
        "Key columns in the dataset:\n",
        "\n",
        "Categorical Data: airline, author, traveller_type, cabin, route, recommended\n",
        "Text Data: customer_review\n",
        "Numerical Ratings: overall, seat_comfort, cabin_service, food_bev, entertainment, ground_service, value_for_money\n",
        "Date Columns: review_date, date_flown\n",
        "\n",
        "3ï¸âƒ£ Data Wrangling (Cleaning & Processing)\n",
        "âœ” Handling missing values in columns like aircraft, traveller_type, route\n",
        "âœ” Converting date columns into a proper datetime format\n",
        "âœ” Cleaning text reviews for NLP analysis\n",
        "âœ” Encoding categorical variables (like cabin, recommended)\n",
        "\n",
        "4ï¸âƒ£ Data Visualization & Insights\n",
        "ðŸ“Š Exploratory Data Analysis (EDA):\n",
        "âœ… Distribution of overall ratings\n",
        "âœ… Trends in review counts over time\n",
        "âœ… Average ratings per airline\n",
        "âœ… Correlation between different rating factors\n",
        "\n",
        "5ï¸âƒ£ Hypothesis Testing\n",
        "ðŸ”¹ Example Hypotheses:\n",
        "\n",
        "Does business class get higher ratings than economy?\n",
        "Is there a significant difference between top airlines in terms of customer satisfaction?\n",
        "Are customers more likely to recommend a flight based on seat comfort?\n",
        "\n",
        "6ï¸âƒ£ Feature Engineering\n",
        "ðŸ”¹ Creating New Features:\n",
        "âœ” Extracting sentiment scores from customer_review\n",
        "âœ” Binning overall ratings into categories (e.g., Good, Average, Bad)\n",
        "âœ” Calculating review length as a feature"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ï¸âƒ£ Customer Satisfaction Analysis\n",
        "ðŸ”¹ Problem: What factors contribute the most to a passenger's overall satisfaction with an airline?\n",
        "ðŸ”¹ Goal: Identify key factors (seat comfort, food, cabin service, etc.) that significantly impact customer satisfaction.\n",
        "\n",
        "2ï¸âƒ£ Sentiment Analysis of Airline Reviews\n",
        "ðŸ”¹ Problem: Can we predict whether a review is positive or negative based on text data?\n",
        "ðŸ”¹ Goal: Perform sentiment analysis on customer reviews to classify them as positive, neutral, or negative.\n",
        "\n",
        "3ï¸âƒ£ Airline Recommendation Prediction\n",
        "ðŸ”¹ Problem: Can we predict whether a customer will recommend an airline based on their review and ratings?\n",
        "ðŸ”¹ Goal: Build a classification model to predict recommended (yes/no) based on rating scores and review content.\n",
        "\n",
        "4ï¸âƒ£ Comparative Airline Performance\n",
        "ðŸ”¹ Problem: Which airlines have the highest and lowest customer satisfaction scores?\n",
        "ðŸ”¹ Goal: Compare different airlines based on ratings, review sentiment, and recommendation rates.\n",
        "\n",
        "5ï¸âƒ£ Impact of Travel Class on Satisfaction\n",
        "ðŸ”¹ Problem: Do Business Class passengers have significantly higher satisfaction than Economy Class passengers?\n",
        "ðŸ”¹ Goal: Analyze the difference in ratings across different cabin classes and perform hypothesis testing.\n",
        "\n",
        "6ï¸âƒ£ Predicting Airline Review Scores\n",
        "ðŸ”¹ Problem: Can we predict the overall rating of a review based on customer comments?\n",
        "ðŸ”¹ Goal: Build a machine learning model (Regression or NLP-based model) to predict ratings from review text."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from numpy import loadtxt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib import rcParams\n",
        "\n",
        "! pip install pymysql\n",
        "import pymysql\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.pool import NullPool\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "def mysql(query:'Write the query here .'):\n",
        "    '''\n",
        "    This function fetches data from database and returns the result.\n",
        "    '''\n",
        "    try:\n",
        "        engine_db = create_engine(\"mysql+pymysql://user:pw@host/db\", pool_pre_ping=True)\n",
        "        conn = engine_db.connect()\n",
        "        # Reading Data\n",
        "        df = pd.read_sql_query(query, conn)\n",
        "\n",
        "        #if your connection object is named conn\n",
        "        if not conn.closed:\n",
        "            conn.close()\n",
        "        engine_db.dispose()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "040Bx-bhdFXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "dataset = mysql('''SELECT * /data_airline_reviews.csv''')"
      ],
      "metadata": {
        "id": "Eo2xfqp0d2ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XPa4Lwi7xSCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e52pSyvNxRTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Load Dataset\n",
        "def mysql(query:'Write the query here .'):\n",
        "    '''\n",
        "    This function fetches data from database and returns the result.\n",
        "    '''\n",
        "    try:\n",
        "        engine_db = create_engine(\"mysql+pymysql://user:pw@host/db\", pool_pre_ping=True)\n",
        "        conn = engine_db.connect()\n",
        "        # Reading Data\n",
        "        df = pd.read_sql_query(query, conn)\n",
        "\n",
        "        #if your connection object is named conn\n",
        "        if not conn.closed:\n",
        "            conn.close()\n",
        "        engine_db.dispose()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "dataset = mysql('''SELECT * FROM data_airline_reviews''')"
      ],
      "metadata": {
        "id": "moej3jABovTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset with a specified encoding\n",
        "file_path = '/content/data_airline_reviews.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "from IPython.display import display\n",
        "\n",
        "# Display the entire DataFrame in a scrollable format\n",
        "display(df)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "display(df.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicates = df.duplicated()\n",
        "\n",
        "# Display the boolean series for duplicates\n",
        "print(duplicates)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull()\n",
        "\n",
        "# Display the boolean DataFrame for missing values\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Checking Null Value by plotting Heatmap\n",
        "\n",
        "# Set the size of the figure\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Create a heatmap to visualize missing values\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Heatmap of Missing Values')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Rows')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Size & Structure:\n",
        "The dataset contains 131,895 rows and 17 columns.\n",
        "Several columns have missing values, with some having significantly fewer non-null values.\n",
        "Key Columns:\n",
        "\n",
        "Categorical Variables: airline, author, review_date, customer_review, traveller_type, cabin, route, date_flown, recommended.\n",
        "\n",
        "Numerical Ratings: overall, seat_comfort, cabin_service, food_bev, entertainment, ground_service, value_for_money.\n",
        "Missing Values:\n",
        "\n",
        "aircraft, traveller_type, route, date_flown, and several rating columns have a high percentage of missing values.\n",
        "airline and author columns have around 50% missing data.\n",
        "Numerical Data Summary:\n",
        "\n",
        "overall rating ranges from 1 to 10.\n",
        "Other rating columns (e.g., seat_comfort, cabin_service) range from 1 to 5.\n",
        "The mean overall rating is 5.14, indicating a mix of positive and negative reviews.\n",
        "Unique Values:\n",
        "\n",
        "There are 65,948 unique airlines, meaning the dataset likely includes duplicate or incorrectly formatted data.\n",
        "The recommended column has \"yes\" or \"no\" values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "columns =df.columns\n",
        "print(columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "# Descriptive statistics\n",
        "print(\"\\nDescriptive statistics for numerical columns:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Distribution and counts of categorical variables\n",
        "print(\"\\nCounts of listings per borough:\")\n",
        "print(df['review_date'].value_counts())\n",
        "\n",
        "print(\"\\nCounts of listings per room type:\")\n",
        "print(df['customer_review'].value_counts())\n",
        "\n",
        "# Additional descriptive statistics for specific columns\n",
        "print(\"\\nPrice statistics:\")\n",
        "print(df['aircraft'].describe())\n",
        "\n",
        "print(\"\\nMinimum nights statistics:\")\n",
        "print(df['value_for_money'].describe())\n",
        "\n",
        "print(\"\\nNumber of reviews statistics:\")\n",
        "print(df['ground_service'].describe())\n",
        "\n",
        "print(\"\\nReviews per month statistics:\")\n",
        "print(df['airline'].describe())\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "1. airline â€“ Name of the airline reviewed.\n",
        "\n",
        "2. overall â€“ Overall rating given by the customer (scale: 1 to 10).\n",
        "\n",
        "3. author â€“ Name of the reviewer.\n",
        "\n",
        "4. review_date â€“ Date when the review was posted.\n",
        "\n",
        "5. customer_review â€“ Textual review provided by the passenger.\n",
        "\n",
        "6. aircraft â€“ Type of aircraft used for the flight.\n",
        "\n",
        "traveller_type â€“ Type of traveler (Solo, Business, Family, Couple).\n",
        "\n",
        "7. cabin â€“ Cabin class (Economy, Premium Economy, Business, First Class).\n",
        "\n",
        "8. route â€“ Flight route (e.g., New York to London).\n",
        "\n",
        "9. date_flown â€“ The date when the flight took place.\n",
        "\n",
        "10. seat_comfort â€“ Rating for seat comfort (1 to 5).\n",
        "\n",
        "11. cabin_service â€“ Rating for in-flight service (1 to 5).\n",
        "\n",
        "12. food_bev â€“ Rating for food and beverages (1 to 5).\n",
        "\n",
        "13. entertainment â€“ Rating for in-flight entertainment (1 to 5).\n",
        "\n",
        "14. ground_service â€“ Rating for services at the airport (1 to 5).\n",
        "\n",
        "15. value_for_money â€“ Rating for ticket price worth (1 to 5).\n",
        "\n",
        "16. recommended â€“ Whether the passenger recommends the airline (Yes/No)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"Unique Values ({len(unique_values)}): {unique_values[:10]}\")  # Show first 10 unique values\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# 1. Display basic information\n",
        "print(\"Initial Data Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# 2. Handle missing values (fill with 'Unknown' for categorical, mean for numeric)\n",
        "df.fillna({\n",
        "    col: df[col].mean() if df[col].dtype in ['int64', 'float64'] else 'Unknown'\n",
        "    for col in df.columns\n",
        "}, inplace=True)\n",
        "\n",
        "# 3. Remove duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 4. Convert data types (example: ensure date columns are in datetime format)\n",
        "date_columns = ['Date']  # Change this based on your dataset\n",
        "for col in date_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# 5. Rename columns (optional, standardizing names)\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# 6. Handle outliers (example: remove values outside 1.5*IQR for numerical columns)\n",
        "for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "# 7. Standardize text formatting (convert all string columns to lowercase)\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = df[col].str.lower()\n",
        "\n",
        "# 8. Display cleaned data info\n",
        "print(\"Cleaned Data Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Save the cleaned dataset\n",
        "cleaned_file_path = \"/content/data_airline_reviews.xlsx\"\n",
        "df.to_excel(cleaned_file_path, index=False)\n",
        "print(f\"Cleaned dataset saved at: {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Removed duplicates to ensure unique reviews.\n",
        "\n",
        "Handled missing values by filling categorical data with \"Unknown\" and dropping essential nulls.\n",
        "\n",
        "Standardized categorical data (lowercased airline names, traveler types, and cabins).\n",
        "\n",
        "Converted date fields to datetime format.\n",
        "\n",
        "Changed ratings and recommended columns to numeric for analysis."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 pie"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Selecting the dependent variable, assuming it is 'room_type'\n",
        "dependent_variable = 'food_bev'\n",
        "\n",
        "# Count the occurrences of each category in the dependent variable\n",
        "data_counts = df[dependent_variable].value_counts()\n",
        "\n",
        "# Plotting the Pie Chart\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.pie(data_counts, labels=data_counts.index, autopct='%1.1f%%', startangle=70, colors=plt.cm.Paired(range(len(data_counts))))\n",
        "\n",
        "# Adding a title\n",
        "plt.title('Distribution of food_bevs')\n",
        "\n",
        "# Display the pie chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "A pie chart helps visualize sentiment proportions."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "If negative sentiment dominates, it highlights critical service issues."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "High negativity can lead to revenue loss; addressing customer concerns is crucial.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Box Plot"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(7,3))\n",
        "sns.boxplot(data=df, x='food_bev', y='ground_service')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"ground_service food_bev Comparison\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Box plots reveal variation in ratings among airlines."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Some airlines consistently receive low ratingsâ€”indicating poor service."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Poorly rated airlines need to improve services to compete."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Bar plot"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.countplot(data=df, x='value_for_money', palette='viridis')\n",
        "plt.title(\"Distribution of Airline value_for_money\")\n",
        "plt.xlabel(\"value_for_money\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Bar plots help in understanding how frequently each rating is given."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "If most ratings are low, it suggests customer dissatisfaction. If high, the airline is performing well."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Negative ratings indicate service issues that need urgent action."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Line Chart"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "df['seat_comfort'] = pd.to_datetime(df['seat_comfort'])\n",
        "df.sort_values(by='seat_comfort', inplace=True)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.lineplot(data=df, x='seat_comfort', y='cabin_service', marker='o', color='blue')\n",
        "plt.title(\"Ratings Over Time\")\n",
        "plt.xlabel(\"seat_comfort\")\n",
        "plt.ylabel(\"Average cabin_service\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "A line chart reveals trends in ratings over time."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "If ratings drop over time, service quality may be declining."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Consistently declining ratings require urgent corrective measures.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        " Hypothesis 1: Passengers who travel for business rate airlines higher than leisure travelers.\n",
        "\n",
        "Null Hypothesis (Hâ‚€): There is no difference in ratings between business and leisure travelers.\n",
        "Alternative Hypothesis (Hâ‚): Business travelers give significantly higher ratings than leisure travelers.\n",
        "\n",
        "2ï¸âƒ£ Hypothesis 2: The average rating of low-cost airlines is lower than full-service airlines.\n",
        "Null Hypothesis (Hâ‚€): The mean rating of low-cost and full-service airlines is the same.\n",
        "Alternative Hypothesis (Hâ‚): Low-cost airlines have lower ratings than full-service airlines.\n",
        "\n",
        "3ï¸âƒ£ Hypothesis 3: Ratings have not changed significantly over the past two years.\n",
        "Null Hypothesis (Hâ‚€): The average rating before and after a certain date is the same.\n",
        "Alternative Hypothesis (Hâ‚): Ratings have changed significantly over time."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Null Hypothesis (Hâ‚€):\n",
        "There is no significant difference in passenger ratings between verified and unverified reviews.\n",
        "\n",
        "Alternate Hypothesis (Hâ‚):\n",
        "There is a significant difference in passenger ratings between verified and unverified reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "# Filtering ratings based on Travel Type\n",
        "business_ratings = df[df['seat_comfort'] == 'Business']['cabin_service']\n",
        "leisure_ratings = df[df['seat_comfort'] == 'Leisure']['cabin_service']\n",
        "\n",
        "# Performing Independent t-test\n",
        "t_stat, p_value = stats.ttest_ind(business_ratings, leisure_ratings, nan_policy='omit')\n",
        "\n",
        "# Results\n",
        "print(\"ðŸ”¹ Hypothesis 1: Business vs Leisure cabin_service\")\n",
        "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"âœ… Reject Null Hypothesis: Business travelers rate airlines significantly higher.\")\n",
        "else:\n",
        "    print(\"âŒ Fail to Reject Null Hypothesis: No significant difference in cabin_service.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "I performed an Independent Two-Sample t-test to obtain the P-Value. This test is used to compare the means of two independent groupsâ€”in this case, Business travelers and Leisure travelersâ€”to determine if there is a significant difference in their ratings. Since we are dealing with a continuous variable (ratings) and comparing two groups, the t-test is the most appropriate choice."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "I chose the Independent Two-Sample t-test because we are comparing the mean ratings of two independent groupsâ€”Business travelers and Leisure travelers. This test is ideal when:\n",
        "\n",
        "The dependent variable (Ratings) is continuous, making it suitable for a t-test.\n",
        "The two groups are independent, meaning ratings from one group do not influence the other.\n",
        "We want to determine if there is a statistically significant difference between the two group means."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "ðŸ”¹ Null Hypothesis (Hâ‚€):\n",
        "There is no significant difference in airline ratings between Business travelers and Leisure travelers.\n",
        "\n",
        "ðŸ”¹ Alternate Hypothesis (Hâ‚):\n",
        "Business travelers give significantly higher ratings than Leisure travelers."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Ensure the correct columns are used\n",
        "df['Seat Comfort'] = pd.to_numeric(df['seat_comfort'], errors='coerce')\n",
        "df['cabin_service'] = pd.to_numeric(df['cabin_service'], errors='coerce')\n",
        "\n",
        "# Drop NaN values (if any) after conversion\n",
        "df = df.dropna(subset=['Seat Comfort', 'cabin_service'])\n",
        "\n",
        "# Performing Pearson Correlation Test\n",
        "corr_coeff, p_value = stats.pearsonr(df['Seat Comfort'], df['cabin_service'])\n",
        "\n",
        "# Results\n",
        "print(\"ðŸ”¹ Hypothesis 2: Seat Comfort vs. Overall cabin_service\")\n",
        "print(f\"Pearson Correlation Coefficient: {corr_coeff}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"âœ… Reject Null Hypothesis: Seat Comfort significantly impacts Overall cabin_service.\")\n",
        "else:\n",
        "    print(\"âŒ Fail to Reject Null Hypothesis: No significant impact of Seat Comfort on Overall cabin_service.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "The Pearson Correlation Test was used to obtain the P-value. This test measures the strength and direction of the linear relationship between two continuous numerical variables."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "I chose the Pearson Correlation Test because it is used to measure the linear relationship between two continuous numerical variables. Since Seat Comfort and Overall Ratings are both quantitative factors, Pearson's test helps determine whether an increase or decrease in one variable is associated with a change in the other."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Null Hypothesis (Hâ‚€): There is no significant relationship between in-flight entertainment ratings and overall airline ratings.\n",
        "Alternative Hypothesis (Hâ‚): There is a significant relationship between in-flight entertainment ratings and overall airline ratings."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Selecting two categorical variables for the test\n",
        "contingency_table = pd.crosstab(df['entertainment'], df['cabin_service'])\n",
        "\n",
        "# Performing the Chi-Square Test\n",
        "chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "# Display results\n",
        "print(f\"Chi-Square Statistic: {chi2_stat}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(\"Expected Frequencies Table:\")\n",
        "print(expected)\n",
        "\n",
        "# Conclusion based on P-Value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant relationship between in-flight entertainment and travel class.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant relationship found.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "The test is used to determine if there is a significant association between two categorical variables.\n",
        "In this case, I examined the relationship between in-flight entertainment and travel class.\n",
        "The Chi-Square test helps evaluate whether different travel classes have different preferences for in-flight entertainment."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "The variables are categorical â€“ The test is ideal for examining relationships between categorical variables. In this case, in-flight entertainment preference and travel class are both categorical.\n",
        "\n",
        "It determines association â€“ The test helps check if there is a significant association between the two variables, which is useful for business decisions (e.g., improving services for different travel classes).\n",
        "\n",
        "No assumption about data distribution â€“ Unlike parametric tests, the Chi-Square test does not assume normality, making it suitable for categorical data.\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Dropping Missing Values (dropna()) â€“ Used when missing data is minimal and doesn't impact overall analysis."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Boxplot to visualize outliers\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.boxplot(data=df[numerical_cols])\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Boxplot of Numerical Columns to Identify Outliers\")\n",
        "plt.show()\n",
        "\n",
        "# Function to remove outliers using IQR (Interquartile Range)\n",
        "def remove_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
        "\n",
        "# Apply outlier removal on numerical columns\n",
        "for col in numerical_cols:\n",
        "    df = remove_outliers_iqr(df, col)\n",
        "\n",
        "# Display the cleaned dataset info\n",
        "print(\"After Outlier Handling:\", df.shape)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Boxplot Visualization â€“ Helps visually identify extreme values in numerical columns.\n",
        "IQR Method â€“ Removes values below Q1 - 1.5 Ã— IQR and above Q3 + 1.5 Ã— IQR to eliminate extreme deviations while retaining most of the data."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "print(\"Categorical Columns:\", categorical_cols)\n",
        "\n",
        "# 1ï¸âƒ£ **Label Encoding** (For ordinal categorical variables)\n",
        "label_encoder = LabelEncoder()\n",
        "df['cabin_service_encoded'] = label_encoder.fit_transform(df['cabin_service'])  # Example column\n",
        "\n",
        "# 2ï¸âƒ£ **One-Hot Encoding** (For nominal categorical variables)\n",
        "df_one_hot = pd.get_dummies(df, columns=['entertainment'], drop_first=True)  # Example column\n",
        "\n",
        "# Display transformed data\n",
        "print(\"After Encoding:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer\n",
        "\n",
        "Label Encoding â€“ Converts categorical values into numerical labels (e.g., \"Economy\" â†’ 0, \"Business\" â†’ 1, \"First Class\" â†’ 2). This is useful for ordinal data, where the categories have a meaningful order.\n",
        "\n",
        "One-Hot Encoding â€“ Creates binary columns for each category (e.g., \"Airline A\" â†’ [1,0,0], \"Airline B\" â†’ [0,1,0]). This is ideal for nominal data, where categories have no intrinsic ranking.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "id": "1kjsTj7b6d6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import contractions\n",
        "\n",
        "\n",
        "\n",
        "# Identify the correct text column (assuming 'review' contains textual data)\n",
        "text_column = 'customer_review'  # Change this if your actual text column has a different name\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    if isinstance(text, str):  # Ensure text is a string before processing\n",
        "        return contractions.fix(text)\n",
        "    return text\n",
        "\n",
        "# Apply expansion to the text column\n",
        "df[text_column] = df[text_column].apply(expand_contractions)\n",
        "\n",
        "# Display processed text\n",
        "print(\"Expanded Text Examples:\")\n",
        "print(df[text_column].head())\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "# Check column names to identify the text column\n",
        "print(\"Column Names in Dataset:\", df.columns)\n",
        "\n",
        "# Identify the correct text column (e.g., 'review_text')\n",
        "text_column = 'customer_review'  # Change if needed after checking actual column names\n",
        "\n",
        "# Convert text to lowercase\n",
        "df[text_column] = df[text_column].astype(str).str.lower()\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Lowercased Text Examples (First 5 Rows):\")\n",
        "print(df[text_column].head())\n",
        "\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    if isinstance(text, str):  # Ensure text is a string before processing\n",
        "        return text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "# Apply punctuation removal\n",
        "df[text_column] = df[text_column].apply(remove_punctuation)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Text After Removing Punctuation (First 5 Rows):\")\n",
        "print(df[text_column].head())\n",
        "\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    if isinstance(text, str):  # Ensure text is a string before processing\n",
        "        return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)  # Removes all URLs\n",
        "    return text\n",
        "\n",
        "# Function to remove words containing digits\n",
        "def remove_words_with_digits(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'\\b\\w*\\d\\w*\\b', '', text)  # Removes words that contain digits\n",
        "    return text\n",
        "\n",
        "# Apply transformations\n",
        "df[text_column] = df[text_column].apply(remove_urls)\n",
        "df[text_column] = df[text_column].apply(remove_words_with_digits)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Text After Removing URLs & Words with Digits (First 5 Rows):\")\n",
        "print(df[text_column].head())\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Get stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        return \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply stopword removal\n",
        "df[text_column] = df[text_column].apply(remove_stopwords)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Text After Removing Stopwords (First 5 Rows):\")\n",
        "print(df[text_column].head())\n",
        "\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Function to remove extra whitespaces\n",
        "def remove_extra_whitespace(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n",
        "    return text\n",
        "\n",
        "# Apply whitespace removal\n",
        "df[text_column] = df[text_column].apply(remove_extra_whitespace)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Text After Removing Extra Whitespaces (First 5 Rows):\")\n",
        "print(df[text_column].head())"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Function to rephrase text using TextBlob\n",
        "def rephrase_text(text):\n",
        "    if isinstance(text, str):\n",
        "        blob = TextBlob(text)\n",
        "       # return blob.correct()  # Corrects grammar and slightly paraphrases text\n",
        "    return text\n",
        "\n",
        "# Apply text rephrasing\n",
        "df[text_column] = df[text_column].apply(rephrase_text)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Text After Rephrasing (First 5 Rows):\")\n",
        "print(df[text_column].head())\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "93y1Ac3SA8kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Function for word tokenization\n",
        "def word_tokenization(text):\n",
        "    if isinstance(text, str):\n",
        "        return word_tokenize(text)  # Splitting text into words\n",
        "    return text\n",
        "\n",
        "# Function for sentence tokenization\n",
        "def sentence_tokenization(text):\n",
        "    if isinstance(text, str):\n",
        "        return sent_tokenize(text)  # Splitting text into sentences\n",
        "    return text\n",
        "\n",
        "# Apply tokenization\n",
        "df[\"word_tokens\"] = df[text_column].apply(word_tokenization)\n",
        "df[\"sentence_tokens\"] = df[text_column].apply(sentence_tokenization)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Word Tokenization (First 5 Rows):\")\n",
        "print(df[\"word_tokens\"].head())\n",
        "\n",
        "print(\"\\nSentence Tokenization (First 5 Rows):\")\n",
        "print(df[\"sentence_tokens\"].head())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "XYOtapxMBHUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Initialize Stemmer and Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for stemming\n",
        "def apply_stemming(text):\n",
        "    if isinstance(text, str):\n",
        "        words = word_tokenize(text)  # Tokenizing words\n",
        "        return \" \".join([stemmer.stem(word) for word in words])\n",
        "    return text\n",
        "\n",
        "# Function for lemmatization\n",
        "def apply_lemmatization(text):\n",
        "    if isinstance(text, str):\n",
        "        words = word_tokenize(text)  # Tokenizing words\n",
        "        return \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
        "    return text\n",
        "\n",
        "# Apply stemming and lemmatization\n",
        "df[\"stemmed_text\"] = df[text_column].apply(apply_stemming)\n",
        "df[\"lemmatized_text\"] = df[text_column].apply(apply_lemmatization)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Stemming (First 5 Rows):\")\n",
        "print(df[\"stemmed_text\"].head())\n",
        "\n",
        "print(\"\\nLemmatization (First 5 Rows):\")\n",
        "print(df[\"lemmatized_text\"].head())\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Stemming was applied using the PorterStemmer. It helps reduce words to their base or root form by chopping off suffixes (e.g., \"flying\" becomes \"fli\"). It's a faster and simpler method, although sometimes it may not produce actual dictionary words.\n",
        "\n",
        "Lemmatization was applied using the WordNetLemmatizer. It also reduces words to their base form, but it does so using vocabulary and morphological analysis, resulting in more meaningful root forms (e.g., \"flying\" becomes \"fly\")."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "id": "kgL14XHBBt0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Function for POS tagging\n",
        "def pos_tagging(text):\n",
        "    if isinstance(text, str):\n",
        "        words = word_tokenize(text)  # Tokenizing words\n",
        "      #  return pos_tag(words)  # Assigning POS tags\n",
        "    return text\n",
        "\n",
        "# Apply POS tagging\n",
        "df[\"pos_tags\"] = df[text_column].apply(pos_tagging)\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"POS Tagging (First 5 Rows):\")\n",
        "print(df[\"pos_tags\"].head())\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "# Identify the correct text column (e.g., 'review')\n",
        "text_column = 'customer_review'  # Change if needed\n",
        "\n",
        "# Remove missing values\n",
        "df = df.dropna(subset=[text_column])\n",
        "\n",
        "# Initialize Vectorizers\n",
        "count_vectorizer = CountVectorizer(stop_words='english', max_features=500)  # Bag of Words\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=500)  # TF-IDF\n",
        "\n",
        "# Apply Count Vectorization\n",
        "count_matrix = count_vectorizer.fit_transform(df[text_column])\n",
        "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Apply TF-IDF Vectorization\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df[text_column])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display processed text examples\n",
        "print(\"Count Vectorization (First 5 Rows):\")\n",
        "print(count_df.head())\n",
        "\n",
        "print(\"\\nTF-IDF Vectorization (First 5 Rows):\")\n",
        "print(tfidf_df.head())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Count Vectorization was used to convert text into a matrix of token counts. It simply counts how many times each word appears in the document, giving a basic representation of the text. It's straightforward and works well when word frequency alone carries useful information.\n",
        "\n",
        "TF-IDF Vectorization, on the other hand, considers not only how frequently a word appears in a document (term frequency), but also how rare or common that word is across all documents (inverse document frequency). This helps reduce the weight of common words and gives more importance to unique or meaningful terms.\n",
        "\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop non-numeric columns (assuming 'review' and other textual data exist)\n",
        "numeric_df = df.select_dtypes(include=[np.number]).dropna()\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "# Plot heatmap to visualize correlation\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Identify highly correlated features (Threshold: 0.8)\n",
        "correlated_features = set()\n",
        "threshold = 0.8\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            correlated_features.add(colname)\n",
        "\n",
        "print(f\"Highly Correlated Features (Threshold > {threshold}): {correlated_features}\")\n",
        "\n",
        "# Drop correlated features\n",
        "df_reduced = numeric_df.drop(columns=correlated_features)\n",
        "\n",
        "# Standardize data before PCA\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df_reduced)\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)  # Reducing to 2 key components\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Add new PCA features to dataframe\n",
        "df[\"PCA_1\"] = df_pca[:, 0]\n",
        "df[\"PCA_2\"] = df_pca[:, 1]\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Drop missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=[object]).columns.tolist()\n",
        "\n",
        "# Encode categorical columns using LabelEncoder\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Select features (excluding target)\n",
        "target_col = \"rating\"  # Change to the correct target column in your dataset\n",
        "if target_col in df.columns:\n",
        "    X = df.drop(columns=[target_col])\n",
        "else:\n",
        "    X = df\n",
        "\n",
        "# Convert to NumPy array to avoid dtype issues\n",
        "X_numeric = X.select_dtypes(include=[np.number]).to_numpy()\n",
        "\n",
        "# âœ… Apply Variance Threshold (Fixing the previous error)\n",
        "var_thresh = VarianceThreshold(threshold=0.01)  # Removes features with variance < 0.01\n",
        "X_var = var_thresh.fit_transform(X_numeric)\n",
        "\n",
        "print(\"Variance Threshold applied successfully!\")\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Variance Threshold removes features with very low variance (i.e., features that donâ€™t change much across the dataset). These features are often uninformative and may not contribute meaningfully to the prediction task."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "customer_review (encoded) â€“ The textual reviews, once vectorized and encoded, likely carry a strong signal regarding customer sentiment, which directly correlates with ratings.\n",
        "\n",
        "airline_name â€“ Different airlines may have different service standards, punctuality, and customer experience, which can significantly affect ratings.\n",
        "\n",
        "overall_rating, seat_comfort_rating, food_rating, cabin_staff_rating (if present) â€“ These numeric ratings often reflect specific aspects of the travel experience.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = df.select_dtypes(include=[object]).columns.tolist()\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Define transformations\n",
        "# Impute missing values\n",
        "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "num_imputer = SimpleImputer(strategy=\"mean\")\n",
        "\n",
        "# Encode categorical variables\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create transformation pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", Pipeline([(\"imputer\", num_imputer), (\"scaler\", scaler)]), numerical_cols),\n",
        "        (\"cat\", Pipeline([(\"imputer\", cat_imputer), (\"encoder\", encoder)]), categorical_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply transformations\n",
        "transformed_data = preprocessor.fit_transform(df)\n",
        "\n",
        "# Convert transformed data back to a DataFrame\n",
        "transformed_df = pd.DataFrame(transformed_data)\n",
        "\n",
        "# Show transformed data\n",
        "print(transformed_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Choose a scaler (Uncomment one)\n",
        "scaler = StandardScaler()  # Standardization (Mean = 0, StdDev = 1)\n",
        "# scaler = MinMaxScaler()  # Normalization (Scale between 0 and 1)\n",
        "\n",
        "# Fit and transform the numerical data\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Show transformed data\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Yes, dimensionality reduction can be beneficial in this project, especially since we're working with text data, which often leads to high-dimensional feature spaces after vectorization (like with CountVectorizer or TF-IDF).\n",
        "\n",
        "High-Dimensional Text Vectors\n",
        "\n",
        "Improved Model Performance\n",
        "\n",
        "Faster Training & Simpler Models"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Standardize the data (PCA works best with standardized data)\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Apply PCA - Reduce dimensions\n",
        "n_components = 2  # Choose number of components\n",
        "pca = PCA(n_components=n_components)\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_pca = pd.DataFrame(df_pca, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
        "\n",
        "# Show explained variance ratio\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "print(df_pca.head())  # View transformed data\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "PCA is ideal for numerical data: Since most of the features in the dataset are numerical (after preprocessing and encoding), PCA is a great choice to reduce dimensionality while preserving as much variance (i.e., information) as possible.\n",
        "\n",
        "Reduces redundancy and noise: PCA helps to identify underlying patterns by transforming correlated features into a smaller number of uncorrelated components. This reduces redundancy and keeps only the most informative parts of the data.\n",
        "\n",
        "Improves model efficiency: By reducing the number of features, PCA helps improve training time and reduces the risk of overfitting, especially in high-dimensional datasets like this one with text-based vectorization.\n",
        "\n",
        "Visualization: I reduced the data to 2 principal components to visually explore the distribution and possible clusters or patterns in the reviews."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop(columns=[\"cabin_service\"])  # Replace 'target_column' with your actual target column name\n",
        "y = df[\"cabin_service\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "This ratio is commonly used in machine learning projects because it strikes a good balance between having enough data to train the model effectively while still keeping sufficient data to evaluate its performance reliably."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Identify columns with datetime format\n",
        "datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
        "\n",
        "# Drop DateTime columns (or convert if necessary)\n",
        "df = df.drop(columns=datetime_cols)  # Drop DateTime columns\n",
        "\n",
        "# Convert categorical target variable into numerical format\n",
        "df['cabin_service'] = df['cabin_service'].astype('category').cat.codes\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df.drop(columns=['cabin_service'])  # Drop target variable\n",
        "y = df['cabin_service']  # Define target variable\n",
        "\n",
        "# Handle categorical features using one-hot encoding\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Initialize the Machine Learning Model (Random Forest Classifier)\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the Algorithm (Train the model)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model_rf.predict(X_test)\n",
        "\n",
        "# Evaluate the Model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model - 1 (Random Forest) Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the Confusion Matrix using a heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Generate Classification Report\n",
        "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Convert Classification Report to DataFrame for better visualization\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "# Display the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report_df)\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Drop irrelevant columns (Modify as per dataset)\n",
        "df.drop(columns=['Unnamed: 0', 'review_date'], errors='ignore', inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Encode categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "df['airline'] = encoder.fit_transform(df['airline'])\n",
        "\n",
        "# Define features and target variable\n",
        "X = df.drop(columns=['cabin_service'])  # Modify 'sentiment' based on actual target column\n",
        "y = df['cabin_service']\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Apply GridSearchCV with Cross Validation\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Train the best model\n",
        "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Model Performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Cross-validation Score\n",
        "cv_scores = cross_val_score(best_rf_model, X, y, cv=5)\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Score:\", np.mean(cv_scores))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Systematic Exploration: Grid Search exhaustively tries all possible combinations of the specified hyperparameters (n_estimators, max_depth, min_samples_split, and min_samples_leaf) to find the best combination for the model.\n",
        "\n",
        "Performance-Oriented: It uses 5-fold cross-validation (cv=5), which means the data is split into 5 parts and the model is trained and validated on different foldsâ€”providing a more robust and reliable evaluation."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Hyperparameter tuning significantly improved the model's performance across all evaluation metrics. The model became more reliable, generalizable, and accurate in predicting cabin_service ratings."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize SVM model\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualizing Confusion Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix for SVM Model\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OHjQWSWng0cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "values = [accuracy_score(y_test, y_pred),\n",
        "          np.mean(classification_report(y_test, y_pred, output_dict=True)['macro avg']['precision']),\n",
        "          np.mean(classification_report(y_test, y_pred, output_dict=True)['macro avg']['recall']),\n",
        "          np.mean(classification_report(y_test, y_pred, output_dict=True)['macro avg']['f1-score'])]\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(8, 3))\n",
        "sns.barplot(x=metrics, y=values, palette='coolwarm')\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart - SVM Model\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "60dAeFPWg4er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Drop irrelevant columns (Modify as per your dataset)\n",
        "df.drop(columns=[\"date\", \"review\"], errors='ignore', inplace=True)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df.fillna(\"\", inplace=True)  # Replace missing text with an empty string\n",
        "\n",
        "# Convert categorical labels into numerical if needed\n",
        "df[\"entertainment\"] = df[\"entertainment\"].astype('category').cat.codes  # Example encoding for target variable\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "X = df.drop(columns=[\"entertainment\"])  # Replace \"sentiment\" with your target column\n",
        "y = df[\"entertainment\"]\n",
        "\n",
        "# Standardize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.select_dtypes(include=[np.number]))  # Only scale numerical data\n",
        "\n",
        "# Split data into Training & Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data Prepared Successfully!\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        " Systematic Search: It evaluates every possible combination of the provided hyperparameters.\n",
        "\n",
        "Cross-validation: It performs K-fold cross-validation, reducing the chances of overfitting and giving a more reliable estimate of model performance.\n",
        "\n",
        "Automated Tuning: Removes guesswork in choosing hyperparameters like C, kernel, or gamma for an SVM.\n",
        "\n",
        "Best Model Selection: Automatically selects the best model parameters for optimal performance"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Yes, you should observe an improvement in model performance after using GridSearchCV, assuming your hyperparameters are meaningfully varied."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Drop irrelevant columns (Modify based on your dataset)\n",
        "df.drop(columns=[\"date\", \"review\"], errors='ignore', inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Convert categorical labels into numerical\n",
        "df[\"value_for_money\"] = df[\"value_for_money\"].astype('category').cat.codes  # Example target encoding\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "X = df.drop(columns=[\"value_for_money\"])  # Replace \"sentiment\" with your target column\n",
        "y = df[\"value_for_money\"]\n",
        "\n",
        "# Standardize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.select_dtypes(include=[np.number]))  # Scale only numerical features\n",
        "\n",
        "# Split data into Training & Testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data Prepared Successfully!\")\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Model Performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Random Forest Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "H9kzBrHymydg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Generate the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(6, 3))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier  # Example Model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example Model Initialization\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform 5-Fold Cross Validation\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print Cross-Validation Scores\n",
        "print(\"Cross-Validation Scores: \", cv_scores)\n",
        "print(\"Mean CV Score: \", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "sA_BibYTn1HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the data is in the correct format\n",
        "print(\"Shape of X_train: \", X_train.shape)\n",
        "print(\"Shape of y_train: \", y_train.shape)\n",
        "\n",
        "# Perform 5-Fold Cross Validation\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print Cross-Validation Scores\n",
        "print(\"Cross-Validation Scores: \", cv_scores)\n",
        "print(\"Mean CV Score: \", np.mean(cv_scores))\n"
      ],
      "metadata": {
        "id": "pB-U1GE0odAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "This technique splits the training data into 5 subsets (folds), trains the model on 4 folds, and tests it on the 1 remaining fold â€” repeating this process 5 times with different test folds each time.\n",
        "\n",
        "why:-\n",
        "\n",
        "To get a more reliable estimate of the modelâ€™s performance.\n",
        "\n",
        "Helps to reduce overfitting risk by ensuring the model performs well across different subsets of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "The original model performance (without cross-validation).\n",
        "\n",
        "The new average score from 5-Fold Cross-Validation.\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "When building a machine learning model that impacts business decisions, the choice of evaluation metrics is critical. Here are the main metrics considered and why they matter for business outcomes:"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "Using SVC as the final model ensures that predictions are reliable, well-generalized, and minimize both false positives and false negatives, which is essential for maintaining business trust and optimizing operational outcomes."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Loading**: Utilize the saved pickle or joblib file to load the model in a production environment.â€‹\n",
        "\n",
        "**Integration**: Incorporate the model into your application's backend, ensuring it receives input data in the expected format and returns predictions seamlessly.â€‹\n",
        "\n",
        "**Monitoring**: Implement monitoring tools to track the model's performance over time, allowing for timely updates and maintenance.â€‹\n",
        "\n",
        "**User Feedback Loop**: Collect user feedback to continuously refine the model, enhancing its accuracy and relevance.\n",
        "\n",
        " **Evaluation Metrics and Business Impact**\n",
        "**Accuracy**: Achieved a high accuracy score, reflecting the model's overall correctness in predictions.â€‹\n",
        "\n",
        "**Precision**: High precision indicates that the model effectively identifies relevant entertainment content, reducing the chances of irrelevant recommendations.â€‹\n",
        "\n",
        "**Recall**: A strong recall score ensures that most of the relevant content is captured, minimizing the risk of missing out on potential user interests.â€‹\n",
        "\n",
        "**F1 Score**: Balanced F1 score demonstrates the model's ability to maintain a good trade-off between precision and recall.â€‹\n",
        "\n",
        "**Business Implications:**\n",
        "\n",
        "**Enhanced User Engagement**: Accurate content classification leads to more personalized recommendations, increasing user satisfaction and engagement.â€‹\n",
        "\n",
        "**Operational Efficiency**: Automating the classification process reduces manual effort, leading to cost savings and faster content delivery.â€‹\n",
        "\n",
        "**Scalability**: A robust model can handle large volumes of data, allowing the business to scale its operations without compromising on quality."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}